{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Transform and Load (ETL) Step\n",
    "\n",
    "We are going to use a custom CNN for object identification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from keras.applications import EfficientNetB3\n",
    "\n",
    "\n",
    "ROOT_DIR = \"datasets\\\\aircraft\"\n",
    "DATASET_DIR = \"..\\\\dataset\\\\crop\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes Mapping\n",
    "\n",
    "We also need to provide a classes mapping, here is how we do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_RAW = [\n",
    "    \"a10\",\n",
    "    \"a400m\",\n",
    "    \"ag600\",\n",
    "    \"ah64\",\n",
    "    \"av8b\",\n",
    "    \"an124\",\n",
    "    \"an22\",\n",
    "    \"an225\",\n",
    "    \"an72\",\n",
    "    \"b1\",\n",
    "    \"b2\",\n",
    "    \"b21\",\n",
    "    \"b52\",\n",
    "    \"be200\",\n",
    "    \"c130\",\n",
    "    \"c17\",\n",
    "    \"c2\",\n",
    "    \"c390\",\n",
    "    \"c5\",\n",
    "    \"ch47\",\n",
    "    \"cl415\",\n",
    "    \"e2\",\n",
    "    \"e7\",\n",
    "    \"ef2000\",\n",
    "    \"f117\",\n",
    "    \"f14\",\n",
    "    \"f15\",\n",
    "    \"f16\",\n",
    "    \"f22\",\n",
    "    \"f35\",\n",
    "    \"f4\",\n",
    "    # IN DATA AS F18\n",
    "    \"f18\",\n",
    "    \"h6\",\n",
    "    \"j10\",\n",
    "    \"j20\",\n",
    "    \"jas39\",\n",
    "    \"jf17\",\n",
    "    \"jh7\",\n",
    "    \"kc135\",\n",
    "    \"kf21\",\n",
    "    \"kj600\",\n",
    "    \"ka27\",\n",
    "    \"ka52\",\n",
    "    \"mq9\",\n",
    "    \"mi24\",\n",
    "    \"mi26\",\n",
    "    \"mi28\",\n",
    "    \"mig29\",\n",
    "    \"mig31\",\n",
    "    \"mirage2000\",\n",
    "    \"p3\",\n",
    "    \"rq4\",\n",
    "    \"rafale\",\n",
    "    \"sr71\",\n",
    "    \"su24\",\n",
    "    \"su25\",\n",
    "    \"su34\",\n",
    "    \"su57\",\n",
    "    \"tb001\",\n",
    "    \"tb2\",\n",
    "    \"tornado\",\n",
    "    \"tu160\",\n",
    "    \"tu22m\",\n",
    "    \"tu95\",\n",
    "    \"u2\",\n",
    "    \"uh60\",\n",
    "    \"us2\",\n",
    "    \"v22\",\n",
    "    \"vulcan\",\n",
    "    \"wz7\",\n",
    "    \"xb70\",\n",
    "    \"y20\",\n",
    "    \"yf23\",\n",
    "    \"z19\"\n",
    "]\n",
    "\n",
    "CLASSES = {model: i for i, model in enumerate(CLASSES_RAW)}\n",
    "\n",
    "def get_class_id(class_str: str):\n",
    "    return CLASSES[class_str.lower()]\n",
    "\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Dataset Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256 # Keras default\n",
    "IMG_HEIGHT = 256 # Keras default\n",
    "IMG_CHANNELS = 3 # RGB\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Dataset\n",
    "\n",
    "We need to first extract the x and y data from the dataset (image paths and labels).\n",
    "\n",
    "### Splitting the Dataset\n",
    "\n",
    "We need to split out dataset into three parts, train, validation and test.\n",
    "\n",
    "We're going to use (80%, 10%, 10%) repartition for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(paths: list[str], classes: list[int], seed: int = None) -> tuple[list[str], list[int], list[str], list[int], list[str], list[int]]:\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "\n",
    "    if not seed:\n",
    "        seed = time.time()\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    for i in tqdm(range(len(paths))):\n",
    "        rand = random.randint(1, 10)\n",
    "\n",
    "        # Validation (10%)\n",
    "        if rand == 9:\n",
    "            x_val.append(paths[i])\n",
    "            y_val.append(classes[i])\n",
    "\n",
    "        # Test (10%)\n",
    "        elif rand == 10:\n",
    "            x_test.append(paths[i])\n",
    "            y_test.append(classes[i])\n",
    "\n",
    "        # Train (80%)\n",
    "        else:\n",
    "            x_train.append(paths[i])\n",
    "            y_train.append(classes[i])\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "def extract_dataset(dataset_dir: str = DATASET_DIR, seed: int = None) -> None:\n",
    "    aircraft_filepaths = []\n",
    "    aircraft_classes = []\n",
    "    for aircraft_dir in tqdm(os.listdir(dataset_dir)):\n",
    "        aircraft_class = get_class_id(aircraft_dir)\n",
    "\n",
    "        dir_path = os.path.join(dataset_dir, aircraft_dir)\n",
    "        \n",
    "        for aircraft_img in os.listdir(dir_path):\n",
    "            aircraft_img_path = os.path.join(dir_path, aircraft_img)\n",
    "\n",
    "            aircraft_filepaths.append(aircraft_img_path)\n",
    "            aircraft_classes.append(aircraft_class)\n",
    "\n",
    "    print(f\"Found {len(aircraft_filepaths)} aircraft images\")\n",
    "\n",
    "    return aircraft_filepaths, aircraft_classes\n",
    "\n",
    "paths, classes = extract_dataset()\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split_dataset(paths, classes)\n",
    "\n",
    "print(f\"Train: {len(x_train)}\")\n",
    "print(f\"Validation: {len(x_val)}\")\n",
    "print(f\"Test: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Dataset\n",
    "\n",
    "We need to transform the dataset into a format that can be used by the model.\n",
    "\n",
    "This includes\n",
    "\n",
    "- Loading the images as tensors\n",
    "- Resizing the images\n",
    "- Normalizing the images (done later by the model)\n",
    "\n",
    "### Lazy Loading and Tensorflow Datasets\n",
    "\n",
    "We're going to use the `tf.data.Dataset` API to load the images lazily.\n",
    "\n",
    "This is done to avoid loading all the images into RAM at once (10GB+ of images).\n",
    "\n",
    "We also need to shuffle the dataset to avoid biasing the model since the images are ordered by class.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- We're going to use a batch size of 32 for now\n",
    "- We're going to use a prefetch buffer, automatically tuned by Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AircraftDataGenerator:\n",
    "    def __init__(self, filepaths: list[str], classes: list[int], batch_size: int = BATCH_SIZE):\n",
    "        self.filepaths = filepaths\n",
    "        self.classes = classes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_image(self, filepath: str) -> tf.Tensor:\n",
    "        image = tf.io.read_file(filepath)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [IMG_WIDTH, IMG_HEIGHT])\n",
    "        return image\n",
    "\n",
    "    def create_dataset(self) -> tf.data.Dataset:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.filepaths, self.classes))\n",
    "\n",
    "        dataset = dataset.shuffle(buffer_size=len(self.filepaths), reshuffle_each_iteration=True)\n",
    "        \n",
    "        dataset = dataset.map(lambda x, y: (self.load_image(x), y), \n",
    "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        \n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "    \n",
    "train_generator = AircraftDataGenerator(x_train, y_train)\n",
    "val_generator = AircraftDataGenerator(x_val, y_val)\n",
    "test_generator = AircraftDataGenerator(x_test, y_test)\n",
    "\n",
    "train_dataset = train_generator.create_dataset()\n",
    "val_dataset = val_generator.create_dataset()\n",
    "test_dataset = test_generator.create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model Tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_augmentation():\n",
    "    \"\"\"Creates a data augmentation sequential model.\"\"\"\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            keras.layers.RandomFlip(\"horizontal\"),\n",
    "            keras.layers.RandomRotation(0.1),\n",
    "        ],\n",
    "        name=\"data_augmentation\"\n",
    "    )\n",
    "\n",
    "def build_base_model(include_data_augmentation=False):\n",
    "    \"\"\"Builds the base model with or without data augmentation.\"\"\"\n",
    "    layers = []\n",
    "    \n",
    "    if include_data_augmentation:\n",
    "        data_augmentation = build_data_augmentation()\n",
    "        layers.append(data_augmentation)\n",
    "    \n",
    "    base_model = EfficientNetB3(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        pooling=\"max\",\n",
    "    )\n",
    "    layers.append(base_model)\n",
    "    layers.append(keras.layers.BatchNormalization(name=\"batch_normalization\"))\n",
    "    layers.append(\n",
    "        keras.layers.Dense(\n",
    "            256, activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.01), name=\"dense_256\"\n",
    "        )\n",
    "    )\n",
    "    layers.append(keras.layers.Dropout(0.2, name=\"dropout_0.2\"))\n",
    "    layers.append(\n",
    "        keras.layers.Dense(len(CLASSES), activation=\"softmax\", name=\"output_layer\")\n",
    "    )\n",
    "    \n",
    "    model = keras.Sequential(layers, name=\"efficientnetb3_model\")\n",
    "    return model\n",
    "\n",
    "def compile_model(model, optimizer='adam', learning_rate=0.001):\n",
    "    \"\"\"Compiles the model with specified optimizer and learning rate.\"\"\"\n",
    "    if optimizer.lower() == 'adamax':\n",
    "        optimizer_instance = keras.optimizers.Adamax(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer_instance = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer_instance,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_class_weights(y_train):\n",
    "    \"\"\"Calculates class weights for handling class imbalance.\"\"\"\n",
    "    y_train_np = np.array(y_train)\n",
    "    class_weights_values = class_weight.compute_class_weight(\n",
    "        \"balanced\", classes=np.unique(y_train_np), y=y_train_np\n",
    "    )\n",
    "    class_weights_dict = dict(enumerate(class_weights_values))\n",
    "    return class_weights_dict\n",
    "\n",
    "def get_callbacks():\n",
    "    \"\"\"Defines callbacks for training.\"\"\"\n",
    "    def lr_decay(epoch):\n",
    "        return 0.001 * math.pow(0.666, epoch)\n",
    "    \n",
    "    lr_decay_callback = keras.callbacks.LearningRateScheduler(lr_decay, verbose=1)\n",
    "    early_stop_callback = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"best_model_sofar.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return [lr_decay_callback, early_stop_callback, checkpoint_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multiple Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_basic():\n",
    "    \"\"\"Trains the basic model without any enhancements.\"\"\"\n",
    "    print(\"Training Basic Model...\")\n",
    "    model = build_base_model(include_data_augmentation=False)\n",
    "    model = compile_model(model, optimizer='adam', learning_rate=0.001)\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"basic_model.keras\")\n",
    "    print(\"Basic Model saved\")\n",
    "    return history\n",
    "\n",
    "def train_adamax():\n",
    "    \"\"\"Trains the model using the Adamax optimizer.\"\"\"\n",
    "    print(\"Training with Adamax Optimizer...\")\n",
    "    model = build_base_model(include_data_augmentation=False)\n",
    "    model = compile_model(model, optimizer='adamax', learning_rate=0.001)\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"adamax_model.keras\")\n",
    "    print(\"Adamax Model saved\")\n",
    "    return history\n",
    "\n",
    "def train_data_augmentation():\n",
    "    \"\"\"Trains the model with data augmentation.\"\"\"\n",
    "    print(\"Training with Data Augmentation...\")\n",
    "    model = build_base_model(include_data_augmentation=True)\n",
    "    model = compile_model(model, optimizer='adam', learning_rate=0.001)\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"data_augmentation_model.keras\")\n",
    "    print(\"Data Augmentation Model saved\")\n",
    "    return history\n",
    "\n",
    "def train_class_weights():\n",
    "    \"\"\"Trains the model with class weights to handle class imbalance.\"\"\"\n",
    "    print(\"Training with Class Weights...\")\n",
    "    model = build_base_model(include_data_augmentation=False)\n",
    "    model = compile_model(model, optimizer='adam', learning_rate=0.001)\n",
    "    model.summary()\n",
    "    \n",
    "    class_weights_dict = get_class_weights(y_train)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"class_weights_model.keras\")\n",
    "    print(\"Class Weights Model saved\")\n",
    "    return history\n",
    "\n",
    "def train_full_no_cb():\n",
    "    \"\"\"Trains the full model with all enhancements and callbacks.\"\"\"\n",
    "    print(\"Training Full Model with All Enhancements (No CallBacks)...\")\n",
    "    model = build_base_model(include_data_augmentation=True)\n",
    "    model = compile_model(model, optimizer='adamax', learning_rate=0.001)\n",
    "    model.summary()\n",
    "    \n",
    "    class_weights_dict = get_class_weights(y_train)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weights_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"full_model_no_cb.keras\")\n",
    "    print(\"Full Model (No CallBacks) saved\")\n",
    "    return history\n",
    "\n",
    "def train_full_cb():\n",
    "    \"\"\"Trains the full model with all enhancements and callbacks.\"\"\"\n",
    "    print(\"Training Full Model with All Enhancements (With CallBacks)...\")\n",
    "    model = build_base_model(include_data_augmentation=True)\n",
    "    model = compile_model(model, optimizer='adamax', learning_rate=0.001)\n",
    "    model.summary()\n",
    "    \n",
    "    class_weights_dict = get_class_weights(y_train)\n",
    "    callbacks = get_callbacks()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"full_model_cb.keras\")\n",
    "    print(\"Full Model (With CallBacks) saved\")\n",
    "    return history\n",
    "\n",
    "def train_optimal():\n",
    "    \"\"\"Trains the full model with all enhancements and callbacks optimally.\"\"\"\n",
    "    print(\"Training Full Model Optimally (30 EPOCHS)\")\n",
    "    model = build_base_model(include_data_augmentation=True)\n",
    "    model = compile_model(model, optimizer=\"adamax\", learning_rate=0.001)\n",
    "    model.summary()\n",
    "\n",
    "    callbacks = get_callbacks()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=30,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    model.evaluate(test_dataset)\n",
    "    model.save(\"full_model_optimal.keras\")\n",
    "    print(\"Full Model Optimal saved\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And... Benchmark the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function that runs all training configurations and logs their performance.\"\"\"\n",
    "    training_functions = [\n",
    "        train_basic,\n",
    "        train_adamax,\n",
    "        train_data_augmentation,\n",
    "        train_class_weights,\n",
    "        train_full_no_cb,\n",
    "        train_full_cb,\n",
    "        train_optimal,\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for train_fn in training_functions:\n",
    "        result = train_fn()\n",
    "        result.history[\"Training Name\"] = train_fn.__name__\n",
    "        results.append(result)\n",
    "    \n",
    "    with open('training_histories.json', 'w') as f:\n",
    "        json.dump([history.history for history in results], f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Code and Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on Remote GPU from University\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "1. **Basic Model Optimizers:**\n",
    "   - The baseline model (`train_basic`) starts with ~43% accuracy and reaches ~90% accuracy on the training set but validation accuracy plateaus at around ~80%.\n",
    "   - The `train_adamax` run shows a significant improvement in both training and validation performance early on, reaching nearly 89%–90% validation accuracy by the end of training. This suggests that the Adamax optimizer is more suitable than the Adam optimizer (with their respective hyperparameters).\n",
    "\n",
    "2. **Data Augmentation (`train_data_augmentation`):**\n",
    "   - The run with data augmentation starts from a lower initial accuracy (completely normal), but the final validation accuracy is around 82%. Data augmentation normally generalizes the model better and reduces overfitting. The slight improvement compared to `train_basic` might become more pronounced if trained for more epochs.\n",
    "\n",
    "3. **Class Weights (`train_class_weights`):**\n",
    "   - Although there is a class imbalance, class weights seems to make training more difficult. The validation accuracy fluctuates widely and doesn’t steadily improve. Maybe it's not optimal with the current optimizer and learning rate.\n",
    "\n",
    "4. **Full Model With No Callbacks (`train_full_no_cb`):**\n",
    "   - This approach, which includes data augmentation, Adamax optimizer, and class weighting or some combination thereof, starts with a low initial accuracy (probably due to complexity and/or augmented data) but eventually reaches a high training accuracy (94%) and decent validation accuracy (~88%). This indicates your enhancements are working, but you might still benefit from a better learning rate schedule or more epochs.\n",
    "\n",
    "5. **Full Model With Callbacks (`train_full_cb`):**\n",
    "   - Using callbacks (learning rate scheduler, early stopping, checkpoints) plus enhancements leads to steady improvements. The final validation accuracy stabilizes around the mid-80s. The learning rate schedule seems effective, as the loss consistently decreases.\n",
    "\n",
    "6. **Optimal Model (`train_optimal`):**\n",
    "   - The optimal model configuration, Adamax trained on 30 EPOCHS with a learning rate scheduler, early stopping, and checkpoints, starts with a lower initial accuracy but reaches a high training accuracy (97%) and a validation accuracy of ~90%. This is a good result. Even though data augmentation slightly reduces accuracy, we keep it because real life images could vary significantly from dataset.\n",
    "\n",
    "## Potential Improvements\n",
    "\n",
    "1. **Longer Training or Fine-Tuning More Layers:**\n",
    "   - EfficientNetB3 is powerful, but with limited experiment epochs (only 10), we might not be fully converging. We can increase experiment epochs to 20 or 30 to see if validation accuracy improves (takes too long to train).\n",
    "   - Experiment with freezing / unfreezing the EfficientNetB3 layers.\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - Test different learning rates. Values tested : 1e-3, 1e-4, 1e-5 on Adam and Adamax optimizers.\n",
    "   - Test different decay rates for the learning rate scheduler.\n",
    "   - Test different L2 regularization rates.\n",
    "   - Test different batch sizes (32, 64, 128).\n",
    "\n",
    "3. **More Aggressive Data Augmentation:**\n",
    "   - Test different data augmentations (contrast, zoom, brightness) to improve generalization.\n",
    "   - Simulating real world conditions (light, weather, lighting, etc.) could help the model generalize better.\n",
    "\n",
    "4. **Class Balance and Class Weights:**\n",
    "   - The dataset is quite unbalanced, so we could experiment with different class weights or other techniques to handle class imbalance.\n",
    "   - However, it seems that class weights might not be the best approach for this dataset as it significantly affects training stability.\n",
    "\n",
    "5. **Transfer Learning Strategies:**\n",
    "   - Test using larger EfficientNet variant (B4 or later) to see if performance improves.\n",
    "   - Test other architectures like ResNet, DenseNet, or Inception.\n",
    "   - Test other pre-trained models like VGG, MobileNet, or Xception.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Transfer learning was the way to go with this dataset.\n",
    "\n",
    "Building a CNN from the ground up did not perform well despite lots of testing, it never got past 30% testing accuracy no matter what I tried.\n",
    "\n",
    "YOLO and Transfer Learning with EfficientNetB3 were the best performing models, with EfficientNetB3 reaching 90% validation and testing accuracy with the optimal configuration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
